{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a75c008-76b0-412b-a5f1-ec2d39acccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import HandGesture as hg\n",
    "import math\n",
    "\n",
    "# all of this is directly taken from AndreMiras github library repo\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ee9e0c-9a6e-4a38-968c-e691a3555cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class handDetector():\n",
    "    def __init__(self, mode=False, maxHands=2, modelComplexity=1, detectionCon=0.5, trackCon=0.5):\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.modelComplexity = int(modelComplexity)\n",
    "        self.detectionCon = float(detectionCon)\n",
    "        self.trackCon = float(trackCon)\n",
    "\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(\n",
    "            static_image_mode=self.mode,\n",
    "            max_num_hands=self.maxHands,\n",
    "            model_complexity=self.modelComplexity,\n",
    "            min_detection_confidence=self.detectionCon,\n",
    "            min_tracking_confidence=self.trackCon\n",
    "        )\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "    def findHands(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handLms in self.results.multi_hand_landmarks:\n",
    "                if draw:\n",
    "                    self.mpDraw.draw_landmarks(img, handLms, self.mpHands.HAND_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, handNo=0, draw=True):\n",
    "        lmList = []\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            myHand = self.results.multi_hand_landmarks[handNo]\n",
    "            for id, lm in enumerate(myHand.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "                if draw:\n",
    "                    cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "        return lmList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dba0dc8-981a-41d6-8a74-f7332884bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "CamWidth, Camheight = 640, 480 # width and height of the cam screen on laptop screen\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, CamWidth) # 3 is the property id here means id of camwidth is 3\n",
    "cap.set(4, Camheight) # same goes for 4 here\n",
    "start = 0\n",
    "\n",
    "detect = handDetector(detectionCon=0.7, trackCon=0.7) # handdetector imported from script & detectionCon=0.7 means it will show only if there is 70% probability of hand \n",
    "\n",
    "\n",
    "# all of this is directly taken from AndreMiras github library repo after importing the above requirements\n",
    "device = AudioUtilities.GetSpeakers()\n",
    "interface = device.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "# volume.GetMute()  (didn't required)\n",
    "# volume.GetMasterVolumeLevel()   (didn't required)\n",
    "volRange = volume.GetVolumeRange()  # volume range here -65.25 is min and 0 is max so at -65.25 volume will be 0 and at 0 it will be 100\n",
    "\n",
    "# stored the min-65.25 and max 0 in varibales\n",
    "minVol = volRange[0] \n",
    "maxVol = volRange[1]\n",
    "vol = 0\n",
    "volBar = 400 # volume bar on side of screen\n",
    "volPer = 0 # volume percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874bce5b-06c7-4f22-8468-b29b37f8fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laksh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = detect.findHands(img)\n",
    "    \n",
    "    l = detect.findPosition(img, draw=False) # it is a landmark list i.e., coordinates returned by function findPosition\n",
    "    if len(l) != 0:\n",
    "        # print(l[4], l[8])  4th and 8th lm positions in each frame i.e., thumb tip(4th lm)and index finger tip(8th lm) given in mediapipe documentation\n",
    "\n",
    "        # as we will be using thumb and index finger for voice control we are storing their coordinates in variables(in list they are stored as {id,x,y})\n",
    "        x1, y1 = l[4][1], l[4][2]\n",
    "        x2, y2 = l[8][1], l[8][2]\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2 # centre of the line between index and thumb\n",
    "\n",
    "        # circle on the given coordinates of 15 radius and (255, 0, 255) color\n",
    "        cv2.circle(img, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
    "        cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
    "    \n",
    "        cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3) # line between them (between thumb and undex finger)\n",
    "        cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "\n",
    "        \n",
    "        # so we need to know the length between these two points to control volume acc to the length of the line between them for which we use hypot fun\n",
    "        length = math.hypot(x2 - x1, y2 - y1)\n",
    "\n",
    "        # Hand range 50 - 300 minimum 50 and maximum 100\n",
    "        # Volume Range -65.25 - 0\n",
    "        # have to convert the above range into -65.25 to 0 one\n",
    "\n",
    "        vol = np.interp(length, [50, 300], [minVol, maxVol]) # when index and thumb are touched then volume will be -65.25 and length will be 50 so minVol and maxVol for opposite\n",
    "        volBar = np.interp(length, [50, 300], [400, 150]) # it shows a volume bar on the side of screen\n",
    "        volPer = np.interp(length, [50, 300], [0, 100])   # volume percentage\n",
    "        # print(int(len), vol)\n",
    "        volume.SetMasterVolumeLevel(vol, None)  # this will set the volume level basically it is the function which will control the volume in pur system\n",
    "\n",
    "        if length < 50:  # when length<50 color is changed to green\n",
    "            cv2.circle(img, (cx, cy), 15, (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "    cv2.rectangle(img, (50, 150), (85, 400), (255, 0, 0), 3) # empty rectangle  \n",
    "    cv2.rectangle(img, (50, int(volBar)), (85, 400), (255, 0, 0), cv2.FILLED) # rectangle filled as we increase the volume or vice versa\n",
    "    cv2.putText(img, f'{int(volPer)} %', (40, 450), cv2.FONT_HERSHEY_COMPLEX, # percentage is shown\n",
    "                1, (255, 0, 0), 3)\n",
    "\n",
    "\n",
    "    curr = time.time()\n",
    "    fps = 1 / (curr - start)\n",
    "    start = curr\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (40, 50), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                1, (255, 0, 0), 3)\n",
    "\n",
    "    if not success: break\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49a46d-c0d8-4b23-b643-ef8bd09889bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
